{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec: # skip-gram model\n",
    "    def __init__(self, n, eta, epochs, window_size):\n",
    "        self.n = n # dimension of word embeddings (dimension of word's vector)\n",
    "        self.eta = eta # learning rate\n",
    "        self.epochs = epochs # number of epochs\n",
    "        self.window = window_size # size of window context\n",
    "    \n",
    "    def generate_training_data(self, tokens, word2id):\n",
    "        VOCAB_SIZE = len(word2id)\n",
    "        tokens_len = len(tokens)\n",
    "        training_data = []\n",
    "        window = self.window\n",
    "        self.v_count = VOCAB_SIZE\n",
    "        print('Started generating data...')\n",
    "        for i in range(tokens_len):\n",
    "            \n",
    "            X = self.one_hot_encode(word2id[tokens[i]], VOCAB_SIZE)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f'i: {i} in progres..')\n",
    "\n",
    "            # left site of the word\n",
    "            if i - window < 0:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = i - window\n",
    "\n",
    "            # right site of the word\n",
    "\n",
    "            if i + window + 1 > tokens_len:\n",
    "                end = tokens_len\n",
    "            else:\n",
    "                end = i + window + 1\n",
    "\n",
    "            idx = range(start, end)\n",
    "            y = []\n",
    "            for j in idx:\n",
    "#                 if i % 1000 == 0:\n",
    "#                     print(f'j: {j}')\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                y.append(self.one_hot_encode(word2id[tokens[j]], VOCAB_SIZE))\n",
    "            training_data.append([X, y])\n",
    "\n",
    "        print(\"END. Now wait for converting list to numpy array..\")\n",
    "        return np.array(training_data, dtype=object)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def one_hot_encode(self, id, vocab_size):\n",
    "        vector = np.zeros(vocab_size)\n",
    "        vector[id] = 1\n",
    "        return list(vector)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = np.dot(self.w1.T, x)\n",
    "        output = np.dot(self.w2.T, hidden)\n",
    "        y_c = self.softmax(output)\n",
    "        return y_c, hidden, output\n",
    "    \n",
    "    def backpropagation(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        # UPDATE WEIGHTS\n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "    \n",
    "    def train(self, training_data, show_progress=True):\n",
    "        print(\"\\n Started training data..\")\n",
    "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))     # embedding matrix\n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))     # context matrix\n",
    "        history = []\n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            self.loss = 0\n",
    "\n",
    "            for target, context in training_data:\n",
    "\n",
    "                    # FORWARD PASS\n",
    "                    y_pred, hidden, output = self.forward(target)\n",
    "\n",
    "                    # CALCULATE ERROR\n",
    "                    EI = []\n",
    "                    for word in context:\n",
    "                        EI.append(np.subtract(y_pred, word))\n",
    "                    EI = np.sum(EI, axis=0)\n",
    " \n",
    "                    # BACKPROPAGATION\n",
    "                    self.backpropagation(EI, hidden, target)\n",
    "\n",
    "                    # CALCULATE LOSS\n",
    "                    self.loss += -np.sum([output[word.index(1)] for word in context]) + len(context) * np.log(np.sum(np.exp(output)))\n",
    "                \n",
    "            history.append(self.loss)\n",
    "            \n",
    "            with open('./word2vec1000.pickle', 'wb') as f:\n",
    "                cPickle.dump(word2vec, f)\n",
    "            \n",
    "            if show_progress:\n",
    "                print(f'Epoch: {i}/{self.epochs}, Loss: {self.loss}')\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    def get_vector(self, word, word2id):\n",
    "        if word in word2id:\n",
    "            w_index = word2id[word]\n",
    "            v_w = self.w1[w_index]\n",
    "            return v_w\n",
    "        else:   \n",
    "            return np.zeros(self.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./train_dataset_1000.pickle\", \"rb\") as f:\n",
    "    train = cPickle.load(f)\n",
    "\n",
    "with open(r\"./test_dataset_1000.pickle\", \"rb\") as f:\n",
    "    test = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for i in range(len(train['text'])):\n",
    "    all_text += train['text'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    for index, word in enumerate(set(tokens)):\n",
    "        word2id[word] = index\n",
    "        id2word[index] = word\n",
    "    \n",
    "    return word2id, id2word\n",
    "\n",
    "word2id, id2word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./word2vec1000.pickle\", \"rb\") as f:\n",
    "    model = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58220191, -0.14602526,  0.04408969, -1.36537482, -0.11065856])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('is', word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    vectorized_sentence = []\n",
    "    for word in sentence.split():\n",
    "        vectorized_sentence.append(model.get_vector(word, word2id))\n",
    "    return vectorized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vector'] = train['text'].apply(vectorize_sentence)\n",
    "test['vector'] = test['text'].apply(vectorize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all vectors the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_same_length(vector):\n",
    "    MAX_LENGTH = 80\n",
    "    VECTOR_DIMENSION = 5\n",
    "    while len(vector) < 80:\n",
    "        vector.append(np.zeros(VECTOR_DIMENSION))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vector'] = train['vector'].apply(make_same_length)\n",
    "test['vector'] = test['vector'].apply(make_same_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>the humor is spread too thin  and much of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0001551065371010146, -0.463123571448007, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>in his fights  jackie often makes use of objec...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.31003215906565873, -0.2873946033355393, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>throughout the first portion of the film we se...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.7460890029261541, -0.7393681205627268, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>if that fails  maybe shell try a little shock ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.580547487639137, -0.2807332000688982, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>the ideas behind blade are good  as is the bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.0001551065371010146, -0.463123571448007, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  tag  \\\n",
       "1998  the humor is spread too thin  and much of the ...    0   \n",
       "488   in his fights  jackie often makes use of objec...    1   \n",
       "783   throughout the first portion of the film we se...    1   \n",
       "997   if that fails  maybe shell try a little shock ...    1   \n",
       "590   the ideas behind blade are good  as is the bas...    1   \n",
       "\n",
       "                                                 vector  \n",
       "1998  [[-0.0001551065371010146, -0.463123571448007, ...  \n",
       "488   [[0.31003215906565873, -0.2873946033355393, -0...  \n",
       "783   [[-0.7460890029261541, -0.7393681205627268, -0...  \n",
       "997   [[-0.580547487639137, -0.2807332000688982, -0....  \n",
       "590   [[-0.0001551065371010146, -0.463123571448007, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    @staticmethod\n",
    "    def minkowskiMetric(v1, v2, m):\n",
    "        distance = 0\n",
    "        for i in range(len(v1)):\n",
    "            distance += abs(v1[i] - v2[i])**m\n",
    "        distance = distance ** (1/m)\n",
    "        return np.sum(distance)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clustering(testSample, X, k, classes):\n",
    "        distances = []\n",
    "        m = 2\n",
    "        # obliczenie odlegosci\n",
    "        for i in range(len(X)):\n",
    "            distances.append((KNN.minkowskiMetric(testSample, X['vector'].iloc[i], m), i)) # do listy distances dodaje (distance, index)\n",
    "            \n",
    "        # posortowanie odleglosci\n",
    "        distances = sorted(distances) #sortuje distance wraz z indexem\n",
    "        \n",
    "        # glosowanie\n",
    "        for i in range(0, k):\n",
    "            classes[X['tag'].iloc[distances[i][1]]] += 1\n",
    "            \n",
    "        #zwrocenie wyniku\n",
    "        return max(classes, key=classes.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 2 - Accuracy is 51.5%\n",
      "K = 3 - Accuracy is 54.25%\n",
      "K = 4 - Accuracy is 52.0%\n",
      "K = 5 - Accuracy is 52.0%\n",
      "K = 6 - Accuracy is 54.75%\n",
      "K = 7 - Accuracy is 52.75%\n"
     ]
    }
   ],
   "source": [
    "for k in range(2, 8):\n",
    "    corrected = 0\n",
    "    for i in range(len(test)):\n",
    "        classes = {0: 0, 1: 0}\n",
    "        result = KNN.clustering(test['vector'].iloc[i], train, k, classes)\n",
    "        if result == test['tag'].iloc[i]:\n",
    "            corrected += 1\n",
    "\n",
    "    accuracy = corrected / len(test) * 100\n",
    "    print(f'K = {k} - Accuracy is {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST RANdom forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train['vector'].tolist())\n",
    "y = train['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1600, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array(test['vector'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = Xtest.reshape(400, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.5%\n"
     ]
    }
   ],
   "source": [
    "corrected = 0\n",
    "for i in range(len(test)):\n",
    "    result = clf.predict([Xtest[i]])\n",
    "    if result == test['tag'].iloc[i]:\n",
    "        corrected += 1\n",
    "\n",
    "print(f'Accuracy: {(corrected/len(test)) * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
